+++
title = "深度神经网络"
date = 2018-05-08T14:19:26+08:00
draft = false

tags = ["Machine Learning"]
categories = ["Machine Learning"]

[header]
image = ""
caption = ""
preview = true

+++

### 训练流程：
1.寻找一个系列的函数集。
2.判断函数的性质是否优秀
3.找出最优秀的函数
#### 如何将卷积神经网络加入进去：
这三个步骤通过卷积神经网络实现，找到神经网络之后，首先判断在训练集上表现是否良好,否则需要更换最好的函数，在训练集上表现良好之后，接着判断该函数在测试集上是否表现良好，如果不良好则说明出现了过拟合，在这种情况下，就要重新进行第一步，更换一个函数的集合。
### 详细的流程：
1.挑选出合适的损失函数。
2.最小批方法
3.新的激活函数
4.适应性的学习率设定
5.利用冲量（momentum）的方法来进行梯度下降
>最小批方法：将训练集的数据分成几个批次来分别训练需要的模型。

#### 挑选合适的损失函数
还是使用手写数字识别的例子，对于一个最终出现的十维的向量来说，应当如何定义它和普通函数之间的误差函数呢？这里PPT上给出了两个误差函数的定义方法，平方误差函数以及交叉熵误差函数。
平方误差函数：
$$
\sum_{i=1}^{10}(y_i-\dot y_i)^2 = 0
$$
交叉熵函数：
$$
-\sum _{i=1}^{10}\dot y_ilny_i = 0
$$
根据实验可以看出，交叉熵函数的最终识别率在87%左右，平方误差函数的识别率最终停留在了11%，所以在使用softmax输出层的时候，请使用交叉熵代价函数。
[交叉熵代价函数简介](http://blog.csdn.net/u012162613/article/details/44239919)
#### 最小批方法
其实这个并不仅仅是机器学习中使用的方法，这个是一种普遍的实验思想，例子仍然是手写数字识别，一般来说，将数据分解成许多的不同的批次。
首先计算第一个批次的损失函数，根据这个损失函数来对于参数进行调整，以此类推，依次计算所有批次的损失函数，这是一个循环，不断重复。
>在THREARO中，对于这个最小批方法有一个参数调用，分别定义了batch size以及epoch，这就是每一个批次的大小以及最终进行这种操作的次数。

在这种情况下，我们并不一定要计算所有的损失函数并且最终将他们最小化，基本的操作就是随机化所有的网络参数并且最终重复上述的流程。在课件中，给出了一个普通的梯度下降收敛方式和小批训练方式的收敛速度，可以看到，在接近收敛点的时候，小批收敛方式的收敛方式是不稳定的(有益还是有害)。

**最小批方法的优势：**传统的梯度下降方式在看到了所有的参数之后之后才进行一个参数调整，但是最小批次方法，举个例子，如果数据分成了20个批次，那么在一次函数中就可以参数调整20次，速度大大的加快了，但是在并行计算的时候并不一定是完全正确的。从训练效果上来看，也是最小批方法的训练效果要好很多，正确率的上升速度非常快，没有批次的方式在多次收敛之后仍然保持着0.1左右的识别率。

**变化训练的数据：**每一次的循环中，虽然仍然是相同的批次以及每个批的大小，但是其中的数据会不断的变化，这个就交给各个深度学习的训练框架来完成，不需要人为干预。

#### 新的激活函数
在平常的情况下，我们加深神经网络的深度，并不意味着一定可以达成更好的识别效果。例如在手写数字识别方面，事实上3层神经网络的效果要远好于9层神经网络。

>Vanishing Gradient Problem
>小规模的梯度，学习很慢，几乎是完全随机的。
>大规模的梯度，学习的很快，常常是收敛的。
>之前的激活函数是一个指数形式的激活函数，在这种条件下，看一看小型的输入以及大型的输入都会带来差不多的结果。所以在2015年，大家开始使用RELU的激活函数形式。

**RELU激活函数：**
$$
\epsilon (z) = zu(z)
$$
也就是说在z<0的时候，输出的函数形式为0，在z>0的时候，输出和输入相等。使用RELU的函数的原因是这种形式的激活函数计算速度很快，同时还有生物写的原因，还有一个原因Infinite sigmoid with different biases，然后就是解决了之前的问题。
通过应用可以了解到，在神经网络层数为9层的时候，使用sigmoid函数的识别准确率为0.11，使用RELU函数的神经网络函数识别准确率为0.96。

**RELU函数的变种：**变种的主要形式是在z<0的时候的函数形式，一般来说也是线性的，整体的来说就是一个分段线性的函数。 

**MAXOUT：**
ReLU函数是maxout函数的一种特殊形式，maxout事实上是一种可学习的激活函数形式，事实上，对于maxout的网络形式，它的激活函数可以是任意形式的凸函数，有多少分段取决于这一个组织中有多少元素。

#### 调整型的学习率：
学习率如果太高(就是一步推进的次数)，那么会导致总体的损失函数在每一步升级的条件下不一定会收敛，但是如g果学习率太低，那么有可能训练速度太慢。
一个简单的，也是通常的想法认为可以在每一个步骤根据不同的参数进行学习率的微调，开头的时候进行大范围的调整，接近结束的时候见效我们的学习率，常见的学习率调节的参数为：
$$
\epsilon ^t=\frac {\epsilon}{(t+1)^{0.5}}
$$

**Adgard更新规则：**
同样是根据所有的历史数据对于学习率进行更新：
$$
\epsilon _w = \frac{\epsilon}{(\sum_{i=0}{t}(g^i)^2)^{0.5}}
$$
>注意：对于所有的参数来说，学习率都是越来越小的。
>导数越小，则学习率越大，反之亦然。

#### 加快学习速度的动量方法
在普通的随机梯度下降法中，每次更新的x的量为$v=-dx*lr$其中的dx是目标函数对于x的一阶导数。
在使用冲量方法的时候，每次x的更新量$v = -dx*lr+v*momentum$就是原本的更新量加上一个上次更新量乘上一个冲量的和。
>从以上的公式可以得知，如果本次更新方向与上次相同，那么上次更新对于本次更新正像加速，如果二者方向相反，那么上次更新对本次更新起一个减缓的作用，就有这样的效果，在学习率比较小的时候，冲量方法可以加速收敛，在学习率比较大的时候，冲量方法可以减缓震荡，减小震荡幅度。

### 过拟合  
#### 定义：
训练集所用的数据和测试集所用的数据存在着区别，而学习目标的达成是根据训练集的数据得到的，同时并不是绝对的在测试集上有着良好的结果。 
#### 减少过拟合的办法：
1.拥有更多的训练数据
2.创造更多的训练数据（MNIST中将图片旋转15度）
#### 为什么会产生过拟合的现象：
在实验中，我们在数据中加入一些噪音（测试数据），得出如下的情况，对于纯净的数据，训练没有收到影响，但是对于有噪声的数据，最终的识别准确率为0.5。
#### 如何减少过拟合现象的发生：
* 更早的停止训练：
在测试集上的和在训练集上的总体损失函数有这两条不同的曲线，在训练集上的数据曲线会不断减小(肯定是这样)，但是与此同时在测试集上的数据却会先减小后增大，所以要及时停止。
* 权重衰减(weight decay)：
我们的大脑会自动剔除神经元之间用处不大的联系，因此，我们对于人工神经网络采取相同的办法来解决这些问题。权重衰减的方法事实上是正则化的一种体现，在损失函数中，权值衰减会成为正则项前面的一个系数，减小不相关的系数之间的联系。
* dropout
 dropout的思想很简单，就是在不同的情况下让某些神经元以一定的概率不工作，这样就解决了某些特征的协同作用导致的过拟合的问题，相当于训练了许多个不同的测试网络，最后拼合成一个完整的测试网络。